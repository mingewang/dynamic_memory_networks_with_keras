import pickle
import numpy as np

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# TODO: Add proper labels.

def get_tokenizer(texts, max_words, save_path=None):
    """
    Creates a tokenizer fitted on texts.
    if save_path is provided it will also save the tokenizer as a pickle object
    Args:
        texts (list) of (str) : each entry in texts corresponds to separate document in str format.
        max_words (int) : the max words that the tokenizer would use.
        save_path (None) or (str): If (str) tokenizer will be saved at (str)
    Returns:
        (keras.preprocessing.text.Tokenizer) fitted on texts
    """
    tokenizer = Tokenizer(num_words=max_words,
                        filters='!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n',
                        lower=True,#
                        split=" ",
                        char_level=False)
    tokenizer.fit_on_texts(texts)

    print('Found %s unique tokens.' % len(tokenizer.word_index))
    if save_path is not None:
        print(f'Saving tokenizer to {save_path}')
        pickle.dump(tokenizer, open(save_path, 'wb'))

    return tokenizer

def tokenize_texts(texts, max_seq, max_words=None, tokenizer=None):
    """
    Recreates each text in texts as a list of tokens, based on the word_index of
    a tokenizer.
    This tokenizer can be provided by an argument or created by the method itself.
    Args:
        texts (list) of (str) : each entry in texts corresponds to separate document in str format.
        max_seq (int) : the maximum sequence size in texts. Data will be padded to that size
        max_words (None) or (int) : the max words that the tokenizer would use if it is created by this method
        tokenizer (None) or (keras.preprocessing.text.Tokenizer) : Tokenizer to use for tokenizing texts
    Return:
        (np.array), (keras.preprocessing.text.Tokenizer), (dict)
    """
    # TODO: Stream loading of new data.
    # TODO: Maybe consider removing? Maybe use for deployment?

    if tokenizer is None:
        tokenizer, word_index = get_tokenizer(np.reshape(t, newshape=-1), max_words)

    sequences = [tokenizer.texts_to_sequences(text) for text in texts]
    tokenized_data = [pad_sequences(sequence, maxlen=max_seq) for sequence in sequences]

    return tokenized_data, tokenizer, tokenizer.word_index

def load_embeddings_index(embeddings_path):
    """
    Takne from the Keras blog.
    Loads a  pre-trained embeddings matrix,
    such as the one from https://nlp.stanford.edu/projects/glove/

    Args:
        embeddings_path (str) : Location of the file holding the embeddings.
    Return:
        (dict) of embeddings
    """

    embeddings_index = {}
    f = open(embeddings_path, 'r')

    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()

    return embeddings_index

def generate_embeddings_matrix(word_index, embeddings_index, emb_dim):
    """
    Taken from the Keras blog.
    Generates an embeddings matrix from a word index from a tokenizer and an
    embeddings index from the pre-trained GloVed
    Args:
        word_index (dict) : Holds the word index generated by a tokenizer
        embeddings_index (dict) : Holds the pre-trained embeddings
        emb_dim (int) : The size of the embedding vectors.
    """
    embedding_matrix = np.zeros((len(word_index) + 1, emb_dim))

    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector
        else:
            # If the word is not in the embeddings dict generate a random fake embedding
            embedding_matrix[i] = np.random.uniform(0.0,1.0,(emb_dim,))
    return embedding_matrix


def get_positional_encoding(max_seq, emb_dim):
    """
    Generates  a positional encoding as described in section
    4.1 in "End to End Memory Networks" (http://arxiv.org/pdf/1503.08895v5.pdf)
    Adapted from https://github.com/domluna/memn2n and
    https://github.com/barronalex/Dynamic-Memory-Networks-in-TensorFlow/
    Args:
        max_seq (int) : the maximum sequence size in texts. Data will be padded to that size
        emb_dim (int) : The size of the embedding vectors.
    Return:
        (np.array) with the positional encoding.
    """

    encoding = np.ones((emb_dim, max_seq), dtype=np.float32)
    ls = max_seq+1
    le = emb_dim+1
    for i in range(1, le):
        for j in range(1, ls):
            encoding[i-1, j-1] = (i - (le-1)/2) * (j - (ls-1)/2)
    encoding = 1 + 4 * encoding / emb_dim / max_seq
    encoding[:, -1] = 1.0 # TODO maybe remove

    return np.transpose(encoding)

def embed_sentences(sentences, tokenizer, emb_matrix, max_seq=None, positional_encoding=None):
    """
    Return an embedded version of sentences.
    Args:
        sentences (list) of (str) : holds one or more sentences tha twill be tokenized and embedded.
        tokenizer (keras.preprocessing.text.Tokenizer) : A pre-trained tokenizer to take care of the embeddings.
        max_seq (None) or (int) : the maximum sequence size in texts. Data will be padded to that size.If None - no padding.
        positional_encoding (None) or (np.array): If not None, it will be applied to the embedded sentences.
    Return:
        (list) of (np.array)
    """

    def embed_sentence(sentence, emb_matrix):
        return [emb_matrix[word_index] for word_index in sentence]

    sentences = tokenizer.texts_to_sequences(sentences)
    if max_seq is not None:
        sentences = pad_sequences(sentences, max_seq)
    sentences = [np.array(embed_sentence(sentence=s, emb_matrix=emb_matrix)) for s in sentences]
    if positional_encoding is not None:
        # to get sentence representation
        sentences = np.sum(np.array(sentences)*positional_encoding, axis=2)

    return np.array(sentences)

# Taken from https://github.com/barronalex/Dynamic-Memory-Networks-in-TensorFlow
def encode_tasks(tasks, tokenizer, task_labels, embeddings_matrix, positional_encoding, max_seq):
    """
    Args:
        tasks (list) of (dict)s : holds all tasks
        tokenizer (keras.preprocessing.text.Tokenizer) - pre-trained tokenizer object used to encode inputs
        task_labels (list) : containing the unique set of answers for all tasks. Used to generate one-hot labels
        embeddings_matrix (np.array) : holds an embeddings_matrix for the dataset
        positional_encoding (np.array) : matrix used for positional_encoding of inputs
        max_seq (int) : the maximum sequence length. Used for padding.

    Return:
        (list) of (dict)s where each dict holds the embedded representations of the tasks in the dataset
    """
    for task in tasks:
        task['C'] = embed_sentences(task['C'],  tokenizer, embeddings_matrix, max_seq, positional_encoding=positional_encoding)
        task['Q'] = embed_sentences([task['Q']], tokenizer, embeddings_matrix, max_seq)[0]
        task['L'] = np.eye(len(task_labels))[task_labels.index(task["A"])]
        task['A'] = embed_sentences([task['A']], tokenizer, embeddings_matrix)[0]
    return tasks

def get_tasks(babi_task_location):
    """
    Retrieves Babi tasks into a readable dictionary form.
    Each task is represented as a dicitonary that contains the task facts,
    the task question, answer, the indices of the supporting facts, and the task
    ID.

    Taken from https://github.com/barronalex/Dynamic-Memory-Networks-in-TensorFlow/

    Args:
        babi_task_location (str) : path to a file containing babi tasks
    Returns:
        (list) of (dict)s holding a readable representation of the tasks
        (list) of the set of class labels.
    """
    # TODO Add onehot label

    print("==> Loading test from %s" % babi_task_location)
    tasks = []
    task = None
    task_counter = 0
    task_labels = set()

    for i, line in enumerate(open(babi_task_location)):
        id = int(line[0:line.find(' ')])

        if id == 1:
            #  C - text; Q - question, A - answer, S - suporting fact, ID - task number
            task = {"C": [], "Q": "", "A": "", "S": "", "ID": ""}
            counter = 0
            id_map = {}

        line = line.strip()
        line = line.replace('.', ' <EOS>')
        line = line[line.find(' ')+1:]

        # if not a question
        if line.find('?') == -1:
            task["C"].append(line)
            id_map[id] = counter
            counter += 1

        else:
            idx = line.find('?')
            tmp = line[idx+1:].split('\t')
            task["Q"] = line[:idx]
            task["A"] = tmp[1].strip()
            task_labels.add(task["A"])
            task["L"] = []
            for num in tmp[2].split():
                task["L"].append(id_map[int(num.strip())])
            task["ID"] = task_counter
            task_counter+=1

            tasks.append(task.copy())

    return tasks, list(task_labels)

def load_dataset(path_to_set, embeddings_path, emb_dim, tokenizer_path=None, max_seq=10 ):
    """
    Where it all comes together. Loads a babi dataset.
    Args:
        path_to_set (str) : path to the babi dataset you want to use
        path_to_embs (str) : path to  a file holding pretrained word embeddings
        emb_dim (int) : size of the word embeddings to use
        tokenizer (keras.preprocessing.text.Tokenizer) - tokenizer object used to encode inputs
    Return:
        (list), (list) , (list), (list): the encoded inputs, questions, answers and one-hot labels ready to be used by the model
    """
    # It iterates TWICE over the dataset, but hey, its not a big dataset.
    # Determine max sequence length. Will be used for question length too
    # Should be fine unless sentences are too long. In that case TODO
    max_seq = max_seq
    # Load the GloVe embeddings
    embeddings_index = load_embeddings_index(embeddings_path=embeddings_path)

    # Get a tokenizer
    if tokenizer_path is None:
        tokenizer = get_tokenizer( texts=open(path_to_set, 'r').readlines(),
                                   max_words=100)
    else:
        tokenizer = pickle.load(open(tokenizer_path), 'rb')

    # Create the embeddings matrix for this dataset.
    emb_matrix = generate_embeddings_matrix( word_index=tokenizer.word_index,
                                             embeddings_index=embeddings_index,
                                             emb_dim=emb_dim)

    # Calculate the positional encoding
    positional_encoding = get_positional_encoding(max_seq=max_seq, emb_dim=emb_dim)

    # Now that all the bs is done, get the tasks
    tasks, task_labels = get_tasks(babi_task_location=path_to_set)
    tasks = encode_tasks( tasks=tasks,
                          task_labels=task_labels,
                          tokenizer=tokenizer,
                          embeddings_matrix=emb_matrix,
                          max_seq=max_seq,
                          positional_encoding=positional_encoding)

    # and make them into convenient lists.
    x = np.squeeze([x['C'] for x in tasks]) # x_i holds all the facts for task_i

    xq = [x['Q'] for x in tasks]
    word_y = [x['A'] for x in tasks] # word_y_i is the embedding for the answer for the task
    one_hot_y = [ list(x['L']).index(1) for x in tasks] # one_hot_y_i is the corresponding one_hot answer


    return x, xq, word_y, one_hot_y, task_labels
